import os
import re
import time
import uuid
import queue
import threading
from typing import List, Tuple, Dict, Any, Optional

import torch
import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import FileResponse
from pydantic import BaseModel, Field, validator
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

# VibeVoice imports
from vibevoice.modular.modeling_vibevoice_inference import VibeVoiceForConditionalGenerationInference
from vibevoice.processor.vibevoice_processor import VibeVoiceProcessor
from transformers.utils import logging

# --- Setup & Configuration ---

logging.set_verbosity_info()
logger = logging.get_logger(__name__)

# --- Rate Limiting & Task Queue ---

limiter = Limiter(key_func=get_remote_address)
# An in-memory queue to hold incoming generation requests.
task_queue = queue.Queue()

# --- Global Variables & Model Loading ---

model: Optional[VibeVoiceForConditionalGenerationInference] = None
processor: Optional[VibeVoiceProcessor] = None
voice_mapper: Optional[Any] = None
tasks: Dict[str, Dict[str, Any]] = {}

# --- Helper Classes (Unchanged) ---

class VoiceMapper:
    def __init__(self, base_path: str = "demo/voices"):
        self.voices_dir = base_path
        self.voice_presets = {}
        self.available_voices = {}
        self.setup_voice_presets()

    def setup_voice_presets(self):
        if not os.path.exists(self.voices_dir):
            logger.warning(f"Voices directory not found at {self.voices_dir}")
            return
        wav_files = [f for f in os.listdir(self.voices_dir) if f.lower().endswith('.wav')]
        for wav_file in wav_files:
            name = os.path.splitext(wav_file)[0]
            self.voice_presets[name] = os.path.join(self.voices_dir, wav_file)
        self.available_voices = {n: p for n, p in self.voice_presets.items() if os.path.exists(p)}
        logger.info(f"Found {len(self.available_voices)} voices in {self.voices_dir}")

    def get_voice_path(self, speaker_name: str) -> str:
        # Exact and partial matching logic
        if speaker_name in self.available_voices: return self.available_voices[speaker_name]
        speaker_lower = speaker_name.lower()
        for name, path in self.available_voices.items():
            if name.lower() in speaker_lower or speaker_lower in name.lower():
                return path
        if self.available_voices: return list(self.available_voices.values())[0]
        raise ValueError("No voice presets available.")

def parse_txt_script(txt_content: str) -> Tuple[List[str], List[str]]:
    # Replace escaped newlines with actual newlines
    txt_content = txt_content.replace('\\n', '\n')
    
    lines = txt_content.strip().split('\n')
    scripts, speaker_numbers = [], []
    speaker_pattern = r'^Speaker\s+(\d+):\s*(.*)$'
    current_speaker, current_text = None, ""
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        match = re.match(speaker_pattern, line, re.IGNORECASE)
        if match:
            if current_speaker is not None and current_text:
                scripts.append(f"Speaker {current_speaker}: {current_text.strip()}")
                speaker_numbers.append(current_speaker)
            current_speaker, current_text = match.group(1).strip(), match.group(2).strip()
        else:
            if current_text:
                current_text += " " + line
            else:
                current_text = line
    
    if current_speaker is not None and current_text:
        scripts.append(f"Speaker {current_speaker}: {current_text.strip()}")
        speaker_numbers.append(current_speaker)
    
    print(scripts)
    print()
    print(speaker_numbers)
    
    return scripts, speaker_numbers


# --- Background Worker for Inference ---

def generation_worker():
    """
    A dedicated worker thread that continuously processes tasks from the queue.
    Implements chunked processing for long audio generation.
    """
    logger.info("Generation worker started.")
    
    def clear_gpu_cache():
        """Clear GPU cache between generations"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
    
    def process_chunk(chunk_scripts, chunk_speakers, speaker_names, cfg_scale):
        """Process a single chunk of scripts"""
        unique_speakers = sorted(list(set(chunk_speakers)), key=int)
        if len(speaker_names) < len(unique_speakers):
            raise ValueError(f"Chunk has {len(unique_speakers)} speakers, but only {len(speaker_names)} names were given.")
        
        name_map = {num: name for num, name in zip(unique_speakers, speaker_names)}
        voice_samples = [voice_mapper.get_voice_path(name_map[num]) for num in unique_speakers]
        
        inputs = processor(
            text=['\n'.join(chunk_scripts)],
            voice_samples=[voice_samples],
            padding=True, 
            return_tensors="pt", 
            return_attention_mask=True
        )
        
        outputs = model.generate(
            **inputs, 
            max_new_tokens=None, 
            cfg_scale=cfg_scale,
            tokenizer=processor.tokenizer, 
            generation_config={'do_sample': False},
            verbose=False
        )
        
        return outputs.speech_outputs[0]
    
    def process_long_script(script_content, speaker_names, cfg_scale, max_chunk_length=300):
        """Process long scripts in chunks and concatenate results"""
        scripts, speaker_numbers = parse_txt_script(script_content)
        if not scripts: 
            raise ValueError("No valid scripts found in the provided text.")
        
        # Calculate optimal chunk size based on script length
        total_lines = len(scripts)
        num_chunks = max(1, (total_lines + max_chunk_length - 1) // max_chunk_length)
        actual_chunk_size = (total_lines + num_chunks - 1) // num_chunks
        
        logger.info(f"Processing {total_lines} lines in {num_chunks} chunks (~{actual_chunk_size} lines per chunk)")
        
        chunks = []
        
        for i in range(0, total_lines, actual_chunk_size):
            chunk_scripts = scripts[i:i + actual_chunk_size]
            chunk_speakers = speaker_numbers[i:i + actual_chunk_size]
            
            if not chunk_scripts:
                continue
                
            logger.info(f"Processing chunk {len(chunks) + 1}/{num_chunks} ({len(chunk_scripts)} lines)")
            
            # Process the chunk
            audio_chunk = process_chunk(chunk_scripts, chunk_speakers, speaker_names, cfg_scale)
            chunks.append(audio_chunk)
            
            # Update progress
            if task_id in tasks:
                tasks[task_id]["completed_chunks"] = len(chunks)
                tasks[task_id]["progress"] = (len(chunks) / num_chunks) * 100
        
        if not chunks:
            raise ValueError("No audio chunks were generated")
            
        # Combine chunks
        combined_audio = torch.cat(chunks, dim=0)
        return combined_audio
    
    while True:
        task_id, script_content, speaker_names, cfg_scale = task_queue.get()
        
        if task_id is None:  # A way to stop the worker thread if needed.
            break
        
        try:
            clear_gpu_cache()
            
            logger.info(f"Worker picked up task {task_id}.")
            tasks[task_id]["status"] = "running"
            start_time = time.time()
            
            # Calculate total chunks for progress tracking
            scripts, _ = parse_txt_script(script_content)
            total_lines = len(scripts)
            max_chunk_length = 300  # Adjust based on your GPU memory
            num_chunks = max(1, (total_lines + max_chunk_length - 1) // max_chunk_length)
            
            # Initialize progress tracking
            tasks[task_id].update({
                "total_chunks": num_chunks,
                "completed_chunks": 0,
                "progress": 0
            })
            
            # Process the script in chunks
            combined_audio = process_long_script(script_content, speaker_names, cfg_scale, max_chunk_length)
            
            generation_time = time.time() - start_time
            logger.info(f"Task {task_id} finished in {generation_time:.2f}s")
            
            # Save the combined audio
            output_dir = "api_outputs"
            os.makedirs(output_dir, exist_ok=True)
            output_path = os.path.join(output_dir, f"{task_id}.wav")
            processor.save_audio(combined_audio, output_path=output_path)
            
            # Clear memory
            del combined_audio
            clear_gpu_cache()
            
            tasks[task_id].update({
                "status": "completed",
                "result_path": output_path,
                "generation_time": generation_time,
                "progress": 100,
                "audio_duration": len(scripts) * 2.5  # Estimated duration in seconds
            })
            
        except Exception as e:
            logger.error(f"Task {task_id} failed: {e}", exc_info=True)
            if task_id in tasks:
                tasks[task_id].update({
                    "status": "failed", 
                    "error": str(e),
                    "progress": 0
                })
            clear_gpu_cache()
        finally:
            task_queue.task_done()

# --- FastAPI App Definition ---

app = FastAPI(title="VibeVoice API", description="API for long-form multi-speaker TTS")
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.on_event("startup")
async def startup_event():
    global model, processor, voice_mapper
    logger.info("Application startup: loading models...")
    
    model_path = "microsoft/VibeVoice-1.5B"
    device = "cuda" if torch.cuda.is_available() else "cpu"

    if model is None:
        voice_mapper = VoiceMapper(base_path="demo/voices")
        processor = VibeVoiceProcessor.from_pretrained(model_path)
        model = VibeVoiceForConditionalGenerationInference.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map=device,
            attn_implementation="sdpa"
        )
        model.eval()
        model.set_ddpm_inference_steps(num_steps=10)
    
    # Start the background worker thread
    worker_thread = threading.Thread(target=generation_worker, daemon=True)
    worker_thread.start()
    
    logger.info("Models loaded and worker thread started.")

# --- Pydantic Models for API I/O ---

MAX_SCRIPT_LENGTH = 5000  # Adjust based on your needs

class GenerationRequest(BaseModel):
    script: str = Field(..., description="The full script")
    speaker_names: List[str] = Field(..., description="List of voice preset names")
    cfg_scale: float = Field(1.3, ge=1.0, le=2.0)
    
    @validator('script')
    def validate_script_length(cls, v):
        lines = v.strip().split('\n')
        if len(lines) > MAX_SCRIPT_LENGTH:
            raise ValueError(f"Script too long. Maximum {MAX_SCRIPT_LENGTH} lines allowed.")
        return v
    
class GenerationResponse(BaseModel):
    task_id: str
    status: str
    message: str
    queue_position: int

class TaskStatus(BaseModel):
    task_id: str
    status: str
    queue_position: Optional[int] = None
    error: Optional[str] = None
    result_path: Optional[str] = None
    generation_time: Optional[float] = None
    progress: Optional[float] = None
    total_chunks: Optional[int] = None
    completed_chunks: Optional[int] = None
    audio_duration: Optional[float] = None

# --- API Endpoints ---

@app.post("/generate", response_model=GenerationResponse, status_code=202)
@limiter.limit("10/minute")
async def generate_audio(request: Request, generation_request: GenerationRequest):
    task_id = str(uuid.uuid4())
    queue_pos = task_queue.qsize() + 1
    # Initialize task with all required fields
    tasks[task_id] = {
        "status": "queued", 
        "queue_position": queue_pos,
        "progress": 0,
        "total_chunks": 0,
        "completed_chunks": 0
    }
    
    task_queue.put((
        task_id,
        generation_request.script,
        generation_request.speaker_names,
        generation_request.cfg_scale
    ))
    
    return {
        "task_id": task_id,
        "status": "queued",
        "message": "Job accepted and placed in queue.",
        "queue_position": queue_pos
    }

@app.get("/status/{task_id}", response_model=TaskStatus)
async def get_task_status(task_id: str):
    task = tasks.get(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    # Dynamically update queue position if still queued
    if task["status"] == "queued":
        try:
            # Find the position in the current queue
            all_queued_tasks = list(task_queue.queue)
            pos = [item[0] for item in all_queued_tasks].index(task_id) + 1
            task["queue_position"] = pos
        except ValueError:
            # Task might have just been picked up, status will update soon
            task["queue_position"] = 0
            
    return {"task_id": task_id, **task}

@app.get("/result/{task_id}")
async def get_result(task_id: str):
    task = tasks.get(task_id)
    if not task: raise HTTPException(status_code=404, detail="Task not found")
    if task["status"] != "completed":
        raise HTTPException(status_code=400, detail=f"Task not complete. Status: {task['status']}")
    result_path = task.get("result_path")
    if not result_path or not os.path.exists(result_path):
        raise HTTPException(status_code=404, detail="Result file not found.")
    return FileResponse(result_path, media_type="audio/wav", filename=os.pathasename(result_path))

@app.get("/voices", response_model=List[str])
async def list_voices():
    if not voice_mapper: raise HTTPException(status_code=503, detail="VoiceMapper not initialized.")
    return list(voice_mapper.available_voices.keys())

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
